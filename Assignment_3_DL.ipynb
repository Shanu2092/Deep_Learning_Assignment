{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f087c7a",
   "metadata": {},
   "source": [
    "Q1. Is it OK to initialize all the weights to the same value as long as that value is selected\n",
    "randomly using He initialization?\n",
    "\n",
    "Ans. Initializing weights to the same value will create a symmetry in neural network and backpropagation won't be able to change the weights,hence the entire network will behave like a single neuron "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362302fc",
   "metadata": {},
   "source": [
    "Q2. Is it OK to initialize the bias terms to 0?\n",
    "\n",
    "Ans. It will make no such difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4387ce48",
   "metadata": {},
   "source": [
    "Q3.  Name three advantages of the SELU activation function over ReLU.\n",
    "\n",
    "Ans. \n",
    "1. SELU can take negative values,helps in vanishing gradient problem\n",
    "2. It has a non zero derivative,which avoids the dying units issue\n",
    "3. Under right conditions,SELU function ensures the model is self normalised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e43db6",
   "metadata": {},
   "source": [
    "Q4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "\n",
    "Ans. \n",
    "1. SELU - It is a good default \n",
    "2. Leaky ReLU - If you need the network to be as fast as possible, you can use one of the leaky ReLU variants.\n",
    "3. ReLU - It has the ability to output precisely zero,which can be useful\n",
    "4. tanh - If the output has to be in between -1 and 1,tanh can be useful\n",
    "5. logistic - For estimating the probability in the output layer\n",
    "6. Softmax - To output probabilities for mutually exclusive classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b15ed5",
   "metadata": {},
   "source": [
    "Q5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?\n",
    "\n",
    "Ans. \n",
    "\n",
    "Momentum hyperparameter set to 0 means high friction,1 means no friction,at 0.9999 there will be less friction ,This means that the optimizer will overshoot, then come back, overshoot again, and oscillate like this many times before stabilizing at the minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6717f0f7",
   "metadata": {},
   "source": [
    "Q6. Name three ways you can produce a sparse model.\n",
    "\n",
    "Ans. \n",
    "1. Set tiny weights to zero\n",
    "2. Apply strong l1 regularizationduring training\n",
    "3. Use the TF model optimizer toolkit API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb47cbe",
   "metadata": {},
   "source": [
    "Q7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?\n",
    "\n",
    "Ans. \n",
    "Dropout slows down the converging process.As per the paper published by Yarin Gal and Zoubin Ghahramani,training a dropout network is mathematically equivalent to aproxximate Bayesian inference.\n",
    "MC Dropout boosts the dropout model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8352ca60",
   "metadata": {},
   "source": [
    "# Q8. Practice training a deep neural network on the CIFAR10 image dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82dd648",
   "metadata": {},
   "source": [
    "### a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f953b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2718ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 activation=\"elu\",\n",
    "                                 kernel_initializer=\"he_normal\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7e585a",
   "metadata": {},
   "source": [
    "### b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "877777a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f127bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7b76749",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-5)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27ee1512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 161s 1us/step\n",
      "170508288/170498071 [==============================] - 161s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65de1c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d02a1911",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31619e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir=./my_cifar10_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7faa35f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 16s 9ms/step - loss: 3.7399 - accuracy: 0.1751 - val_loss: 2.1250 - val_accuracy: 0.2300\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 2.0022 - accuracy: 0.2623 - val_loss: 1.9714 - val_accuracy: 0.2730\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.8879 - accuracy: 0.3080 - val_loss: 1.8570 - val_accuracy: 0.3244\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.8173 - accuracy: 0.3375 - val_loss: 1.7698 - val_accuracy: 0.3454\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.7633 - accuracy: 0.3602 - val_loss: 1.7969 - val_accuracy: 0.3442\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.7166 - accuracy: 0.3758 - val_loss: 1.7012 - val_accuracy: 0.3800\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.6808 - accuracy: 0.3908 - val_loss: 1.7062 - val_accuracy: 0.3804\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.6461 - accuracy: 0.4036 - val_loss: 1.6733 - val_accuracy: 0.3910\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.6122 - accuracy: 0.4157 - val_loss: 1.6938 - val_accuracy: 0.3800\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.5859 - accuracy: 0.4262 - val_loss: 1.6728 - val_accuracy: 0.3940\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.5612 - accuracy: 0.4349 - val_loss: 1.6807 - val_accuracy: 0.4034\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.5437 - accuracy: 0.4410 - val_loss: 1.6308 - val_accuracy: 0.4162\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.5243 - accuracy: 0.4533 - val_loss: 1.5935 - val_accuracy: 0.4302\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.5041 - accuracy: 0.4592 - val_loss: 1.5568 - val_accuracy: 0.4434\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.4899 - accuracy: 0.4609 - val_loss: 1.5745 - val_accuracy: 0.4366\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.4736 - accuracy: 0.4683 - val_loss: 1.5911 - val_accuracy: 0.4332\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.4587 - accuracy: 0.4736 - val_loss: 1.5220 - val_accuracy: 0.4622\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.4454 - accuracy: 0.4784 - val_loss: 1.5855 - val_accuracy: 0.4280\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.4317 - accuracy: 0.4840 - val_loss: 1.5285 - val_accuracy: 0.4548\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.4174 - accuracy: 0.4901 - val_loss: 1.5131 - val_accuracy: 0.4608\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.4052 - accuracy: 0.4936 - val_loss: 1.5218 - val_accuracy: 0.4590\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3907 - accuracy: 0.4983 - val_loss: 1.4994 - val_accuracy: 0.4712\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3795 - accuracy: 0.5043 - val_loss: 1.5254 - val_accuracy: 0.4718\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3679 - accuracy: 0.5083 - val_loss: 1.5408 - val_accuracy: 0.4542\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.3599 - accuracy: 0.5106 - val_loss: 1.5313 - val_accuracy: 0.4614\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3456 - accuracy: 0.5138 - val_loss: 1.4992 - val_accuracy: 0.4684\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3373 - accuracy: 0.5175 - val_loss: 1.4850 - val_accuracy: 0.4694\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3240 - accuracy: 0.5227 - val_loss: 1.5606 - val_accuracy: 0.4526\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3174 - accuracy: 0.5281 - val_loss: 1.4915 - val_accuracy: 0.4788\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3052 - accuracy: 0.5284 - val_loss: 1.5424 - val_accuracy: 0.4664\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2975 - accuracy: 0.5340 - val_loss: 1.5237 - val_accuracy: 0.4638\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2887 - accuracy: 0.5392 - val_loss: 1.4889 - val_accuracy: 0.4700\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.2768 - accuracy: 0.5390 - val_loss: 1.5966 - val_accuracy: 0.4476\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2672 - accuracy: 0.5442 - val_loss: 1.4849 - val_accuracy: 0.4808\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2585 - accuracy: 0.5481 - val_loss: 1.5174 - val_accuracy: 0.4752\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.2552 - accuracy: 0.5469 - val_loss: 1.5119 - val_accuracy: 0.4758\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.2414 - accuracy: 0.5522 - val_loss: 1.4960 - val_accuracy: 0.4804\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2358 - accuracy: 0.5545 - val_loss: 1.4925 - val_accuracy: 0.4780\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2258 - accuracy: 0.5558 - val_loss: 1.5544 - val_accuracy: 0.4620\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2206 - accuracy: 0.5618 - val_loss: 1.5098 - val_accuracy: 0.4790\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2105 - accuracy: 0.5632 - val_loss: 1.5112 - val_accuracy: 0.4872\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2023 - accuracy: 0.5685 - val_loss: 1.5134 - val_accuracy: 0.4806\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.1933 - accuracy: 0.5713 - val_loss: 1.5340 - val_accuracy: 0.4788\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.1850 - accuracy: 0.5736 - val_loss: 1.5307 - val_accuracy: 0.4724\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.1779 - accuracy: 0.5766 - val_loss: 1.5332 - val_accuracy: 0.4772\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.1708 - accuracy: 0.5772 - val_loss: 1.5678 - val_accuracy: 0.4702\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.1601 - accuracy: 0.5830 - val_loss: 1.5650 - val_accuracy: 0.4808\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1534 - accuracy: 0.5848 - val_loss: 1.5151 - val_accuracy: 0.4870\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.1478 - accuracy: 0.5854 - val_loss: 1.5602 - val_accuracy: 0.4764\n",
      "Epoch 50/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.1388 - accuracy: 0.5904 - val_loss: 1.5362 - val_accuracy: 0.4774\n",
      "Epoch 51/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.1304 - accuracy: 0.5933 - val_loss: 1.5392 - val_accuracy: 0.4854\n",
      "Epoch 52/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.1251 - accuracy: 0.5958 - val_loss: 1.5357 - val_accuracy: 0.4858\n",
      "Epoch 53/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.1202 - accuracy: 0.5976 - val_loss: 1.5487 - val_accuracy: 0.4932\n",
      "Epoch 54/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.1084 - accuracy: 0.6028 - val_loss: 1.5477 - val_accuracy: 0.4764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24b7dba2c10>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7b35024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 1.4849 - accuracy: 0.4808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4848878383636475, 0.48080000281333923]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d5e70",
   "metadata": {},
   "source": [
    "### 3. Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5dce83c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e290c2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 28s 15ms/step - loss: 1.8461 - accuracy: 0.3368 - val_loss: 1.6690 - val_accuracy: 0.4062\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.6716 - accuracy: 0.4047 - val_loss: 1.5697 - val_accuracy: 0.4356\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.6027 - accuracy: 0.4304 - val_loss: 1.5443 - val_accuracy: 0.4456\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.5513 - accuracy: 0.4468 - val_loss: 1.5078 - val_accuracy: 0.4570\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.5060 - accuracy: 0.4658 - val_loss: 1.4446 - val_accuracy: 0.4804\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.4709 - accuracy: 0.4799 - val_loss: 1.4118 - val_accuracy: 0.4986\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.4348 - accuracy: 0.4886 - val_loss: 1.4356 - val_accuracy: 0.4910\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.4058 - accuracy: 0.5008 - val_loss: 1.3867 - val_accuracy: 0.4974\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3816 - accuracy: 0.5128 - val_loss: 1.3551 - val_accuracy: 0.5218\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.3600 - accuracy: 0.5186 - val_loss: 1.3422 - val_accuracy: 0.5218\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.3422 - accuracy: 0.5255 - val_loss: 1.3375 - val_accuracy: 0.5254\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3170 - accuracy: 0.5369 - val_loss: 1.3647 - val_accuracy: 0.5108\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2963 - accuracy: 0.5410 - val_loss: 1.3634 - val_accuracy: 0.5238\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2793 - accuracy: 0.5475 - val_loss: 1.3345 - val_accuracy: 0.5308\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2621 - accuracy: 0.5536 - val_loss: 1.3634 - val_accuracy: 0.5248\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2502 - accuracy: 0.5604 - val_loss: 1.3701 - val_accuracy: 0.5244\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2313 - accuracy: 0.5653 - val_loss: 1.3253 - val_accuracy: 0.5360\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2174 - accuracy: 0.5691 - val_loss: 1.3293 - val_accuracy: 0.5332\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2016 - accuracy: 0.5736 - val_loss: 1.3493 - val_accuracy: 0.5250\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1914 - accuracy: 0.5806 - val_loss: 1.3411 - val_accuracy: 0.5404\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1777 - accuracy: 0.5859 - val_loss: 1.3630 - val_accuracy: 0.5332\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1619 - accuracy: 0.5908 - val_loss: 1.3483 - val_accuracy: 0.5310\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1559 - accuracy: 0.5953 - val_loss: 1.3355 - val_accuracy: 0.5372\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1355 - accuracy: 0.6010 - val_loss: 1.3344 - val_accuracy: 0.5432\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1261 - accuracy: 0.6055 - val_loss: 1.3417 - val_accuracy: 0.5378\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1145 - accuracy: 0.6082 - val_loss: 1.3309 - val_accuracy: 0.5406\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1032 - accuracy: 0.6129 - val_loss: 1.3400 - val_accuracy: 0.5378\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0953 - accuracy: 0.6169 - val_loss: 1.3463 - val_accuracy: 0.5336\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0800 - accuracy: 0.6219 - val_loss: 1.3528 - val_accuracy: 0.5404\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0739 - accuracy: 0.6222 - val_loss: 1.3450 - val_accuracy: 0.5482\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0586 - accuracy: 0.6263 - val_loss: 1.3536 - val_accuracy: 0.5350\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0528 - accuracy: 0.6289 - val_loss: 1.3777 - val_accuracy: 0.5342\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.0373 - accuracy: 0.6354 - val_loss: 1.3259 - val_accuracy: 0.5508\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0309 - accuracy: 0.6355 - val_loss: 1.3393 - val_accuracy: 0.5492\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0205 - accuracy: 0.6381 - val_loss: 1.3642 - val_accuracy: 0.5456\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0118 - accuracy: 0.6449 - val_loss: 1.3824 - val_accuracy: 0.5410\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 0.9969 - accuracy: 0.6492 - val_loss: 1.3580 - val_accuracy: 0.5504\n",
      "157/157 [==============================] - 1s 2ms/step - loss: 1.3253 - accuracy: 0.5360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.325295090675354, 0.5360000133514404]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"elu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_bn_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_bn_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_bn_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be98c1c",
   "metadata": {},
   "source": [
    "As per the outputs,the model converges faster,has better accuracy and better training speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef14255b",
   "metadata": {},
   "source": [
    "### d. Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d29ee90f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 27s 16ms/step - loss: 1.9028 - accuracy: 0.3216 - val_loss: 1.8474 - val_accuracy: 0.3482\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.6984 - accuracy: 0.3999 - val_loss: 1.7947 - val_accuracy: 0.3474\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.6019 - accuracy: 0.4346 - val_loss: 1.7130 - val_accuracy: 0.3970\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.5365 - accuracy: 0.4613 - val_loss: 1.6063 - val_accuracy: 0.4456\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.4802 - accuracy: 0.4825 - val_loss: 1.5612 - val_accuracy: 0.4598\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.4378 - accuracy: 0.4985 - val_loss: 1.5162 - val_accuracy: 0.4764\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.3913 - accuracy: 0.5130 - val_loss: 1.5164 - val_accuracy: 0.4646\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.3511 - accuracy: 0.5289 - val_loss: 1.5060 - val_accuracy: 0.4752\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.3169 - accuracy: 0.5411 - val_loss: 1.5019 - val_accuracy: 0.4760\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.2862 - accuracy: 0.5548 - val_loss: 1.4801 - val_accuracy: 0.4940\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.2527 - accuracy: 0.5647 - val_loss: 1.5412 - val_accuracy: 0.4780\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.2225 - accuracy: 0.5761 - val_loss: 1.4797 - val_accuracy: 0.4886\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.1950 - accuracy: 0.5870 - val_loss: 1.4856 - val_accuracy: 0.5064\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.1753 - accuracy: 0.5909 - val_loss: 1.4951 - val_accuracy: 0.5038\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.1491 - accuracy: 0.6016 - val_loss: 1.5279 - val_accuracy: 0.5060\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.1228 - accuracy: 0.6102 - val_loss: 1.5121 - val_accuracy: 0.5142\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.0953 - accuracy: 0.6228 - val_loss: 1.5514 - val_accuracy: 0.5012\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.0854 - accuracy: 0.6264 - val_loss: 1.5266 - val_accuracy: 0.5100\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.0594 - accuracy: 0.6348 - val_loss: 1.5499 - val_accuracy: 0.5170\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.0483 - accuracy: 0.6422 - val_loss: 1.5574 - val_accuracy: 0.5036\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.0228 - accuracy: 0.6501 - val_loss: 1.5682 - val_accuracy: 0.5020\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.9996 - accuracy: 0.6586 - val_loss: 1.5585 - val_accuracy: 0.5004\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.9768 - accuracy: 0.6650 - val_loss: 1.5622 - val_accuracy: 0.4940\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.9794 - accuracy: 0.6676 - val_loss: 1.5562 - val_accuracy: 0.4946\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.9553 - accuracy: 0.6748 - val_loss: 1.5503 - val_accuracy: 0.5084\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.9342 - accuracy: 0.6820 - val_loss: 1.6446 - val_accuracy: 0.5074\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 709s 505ms/step - loss: 0.9343 - accuracy: 0.6816 - val_loss: 1.5864 - val_accuracy: 0.5084\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 0.9117 - accuracy: 0.6903 - val_loss: 1.6282 - val_accuracy: 0.4898\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.8965 - accuracy: 0.6950 - val_loss: 1.5946 - val_accuracy: 0.5034\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 0.8834 - accuracy: 0.7010 - val_loss: 1.6607 - val_accuracy: 0.4932\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.8625 - accuracy: 0.7063 - val_loss: 1.6911 - val_accuracy: 0.5026\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.8477 - accuracy: 0.7130 - val_loss: 1.7228 - val_accuracy: 0.5020\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 1.4797 - accuracy: 0.4886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.479697346687317, 0.4885999858379364]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=7e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_selu_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_selu_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfb221f",
   "metadata": {},
   "source": [
    "### e. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79359406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 28s 16ms/step - loss: 1.8815 - accuracy: 0.3278 - val_loss: 1.7211 - val_accuracy: 0.4020\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.6534 - accuracy: 0.4176 - val_loss: 1.6282 - val_accuracy: 0.4244\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.5634 - accuracy: 0.4507 - val_loss: 1.6071 - val_accuracy: 0.4334\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.5011 - accuracy: 0.4714 - val_loss: 1.5902 - val_accuracy: 0.4504\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.4433 - accuracy: 0.4918 - val_loss: 1.5304 - val_accuracy: 0.4636\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.3919 - accuracy: 0.5127 - val_loss: 1.5480 - val_accuracy: 0.4864\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.3560 - accuracy: 0.5258 - val_loss: 1.5320 - val_accuracy: 0.4854\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.3157 - accuracy: 0.5400 - val_loss: 1.4859 - val_accuracy: 0.4964\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.2770 - accuracy: 0.5566 - val_loss: 1.4942 - val_accuracy: 0.4918\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.2459 - accuracy: 0.5671 - val_loss: 1.5197 - val_accuracy: 0.5060\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.2124 - accuracy: 0.5801 - val_loss: 1.5773 - val_accuracy: 0.5022\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.1874 - accuracy: 0.5881 - val_loss: 1.5308 - val_accuracy: 0.5014\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.1554 - accuracy: 0.5995 - val_loss: 1.5327 - val_accuracy: 0.5048\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.1303 - accuracy: 0.6106 - val_loss: 1.5517 - val_accuracy: 0.5138\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.1111 - accuracy: 0.6172 - val_loss: 1.5888 - val_accuracy: 0.5076\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.0853 - accuracy: 0.6274 - val_loss: 1.6145 - val_accuracy: 0.5080\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.0606 - accuracy: 0.6362 - val_loss: 1.6028 - val_accuracy: 0.5130\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.0411 - accuracy: 0.6436 - val_loss: 1.6190 - val_accuracy: 0.5106\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.0109 - accuracy: 0.6516 - val_loss: 1.7037 - val_accuracy: 0.5156\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.9968 - accuracy: 0.6607 - val_loss: 1.7084 - val_accuracy: 0.5026\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.9766 - accuracy: 0.6663 - val_loss: 1.6989 - val_accuracy: 0.5098\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.9577 - accuracy: 0.6748 - val_loss: 1.5839 - val_accuracy: 0.5148\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.9410 - accuracy: 0.6785 - val_loss: 1.7262 - val_accuracy: 0.5128\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.9230 - accuracy: 0.6872 - val_loss: 1.6928 - val_accuracy: 0.5184\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.9009 - accuracy: 0.6927 - val_loss: 1.8381 - val_accuracy: 0.5066\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.8953 - accuracy: 0.6975 - val_loss: 1.6995 - val_accuracy: 0.5026\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.8711 - accuracy: 0.7041 - val_loss: 1.6940 - val_accuracy: 0.5130\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.8543 - accuracy: 0.7123 - val_loss: 1.7396 - val_accuracy: 0.5036\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 1.4859 - accuracy: 0.4964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4859399795532227, 0.49639999866485596]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_alpha_dropout_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_alpha_dropout_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_alpha_dropout_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e38b69a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42fe7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3529f1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f76908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

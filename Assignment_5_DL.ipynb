{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3c3aeaf0",
   "metadata": {},
   "source": [
    "1. Why would you want to use the Data API?\n",
    "\n",
    "Ingesting a large dataset and preprocesing can be trickier to implement with other deep learning modules,Data API makes it easier."
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf35d8da",
   "metadata": {},
   "source": [
    "2. What are the benefits of splitting a large dataset into multiple files?\n",
    "\n",
    "Splitting a large dataset into small files makes it easier to shuffle,it's also easier to deal with small files since massively large datasets are not easy to store in the local systems. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba325870",
   "metadata": {},
   "source": [
    "3.During training, how can you tell that your input pipeline is the bottleneck? What can you do to fix it?\n",
    "\n",
    "Using the TF Board,one can check if the GPU is under-utilised or the CPU is taking more time than it is supposed to take,that is when we get to know if the input pipeline is bottleneck or not.\n",
    "                                    One immediate solution is to switch to a more powerfull device,alternatively,we can use cloud services .\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a169514",
   "metadata": {},
   "source": [
    "4.Can you save any binary data to a TFRecord file, or only serialized protocol buffers?\n",
    "\n",
    "Each record can use any binary format but usually ptotobuffs are used becauase they are portable,extensive,and efficient"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e1b598d6",
   "metadata": {},
   "source": [
    "5. Why would you go through the hassle of converting all your data to the Example protobuf format? Why not use your own protobuf definition?\n",
    "\n",
    "An example protobuf format allows you to parse it without having to create your own function,even if that's insufficient,there's protoc for a customised protobuf."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a10fa466",
   "metadata": {},
   "source": [
    "6. When using TFRecords, when would you want to activate compression? Why not do it systematically?\n",
    "\n",
    "Compressio is activated only if TFRecord files are to be downloaded by trianing scripts.But if the files are located on the same machine as the training script,compression will be a waste of CPU."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1750b3d9",
   "metadata": {},
   "source": [
    "7. Data can be preprocessed directly when writing the data files, or within the tf.data pipeline, or in preprocessing layers within your model, or using TF Transform. Can you list a few pros and cons of each option?\n",
    "\n",
    "a. Directly when writing the data files - the training speed will run faster,download speed will be better but you will have to add preprocessing code to the application.\n",
    "\n",
    "b. Within the tf.data pipeline - much easier to tweak the preprocessing logic, apply data augmentation and it's also easy to build preprocessing pipelines,but it will eventually slow down training.\n",
    "\n",
    "c. In preprocessing layers within your model - This way,you will have to write the preprocessing code only once for both training and testing.In this way though,GPU won't be able to do parallel processing .\n",
    "\n",
    "d. Using TF Transform - In this case, the preprocessed data is materialized,each instance is preprocessed just once and the preprocessing layers get generated automatically.The main drawback is that you hav eti learn these tools."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
